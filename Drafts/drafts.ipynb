{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qutip\n",
    "from matplotlib import pyplot, animation\n",
    "\n",
    "\n",
    "# Function to calculate the Bloch vector from a quantum state\n",
    "def state_to_bloch_vector(state):\n",
    "    # Pauli matrices\n",
    "    sx = qutip.sigmax()\n",
    "    sy = qutip.sigmay()\n",
    "    sz = qutip.sigmaz()\n",
    "\n",
    "    # Calculate the expectation values <σx>, <σy>, <σz>\n",
    "    bloch_x = qutip.expect(sx, state)\n",
    "    bloch_y = qutip.expect(sy, state)\n",
    "    bloch_z = qutip.expect(sz, state)\n",
    "\n",
    "    # Return the Bloch vector (x, y, z)\n",
    "    return [bloch_x, bloch_y, bloch_z]\n",
    "\n",
    "\n",
    "# Function to animate the Bloch sphere with a list of states\n",
    "def animate_bloch_trajectory(\n",
    "    states, theta=np.pi / 4\n",
    "):  # Define theta here (e.g., 45 degrees)\n",
    "    # Create the figure and 3D plot\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    sphere = qutip.Bloch(axes=ax)\n",
    "\n",
    "    # Convert each state to a Bloch vector\n",
    "    bloch_vectors = [state_to_bloch_vector(qutip.Qobj(state)) for state in states]\n",
    "\n",
    "    # Define the update function for animation\n",
    "    def animate(i):\n",
    "        sphere.clear()\n",
    "        # Add a reference vector in the direction of theta (optional)\n",
    "        sphere.add_vectors([np.sin(theta), 0, np.cos(theta)])\n",
    "        # Add the trajectory points up to step i\n",
    "        sphere.add_points([bv[: i + 1] for bv in zip(*bloch_vectors)])\n",
    "        sphere.make_sphere()\n",
    "        return ax\n",
    "\n",
    "    # Create the animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(bloch_vectors), blit=False, repeat=False\n",
    "    )\n",
    "\n",
    "    # Save the animation as an mp4 file\n",
    "    ani.save(\"bloch_sphere_trajectory.mp4\", fps=20)\n",
    "\n",
    "\n",
    "# Example: List of quantum states to animate\n",
    "def create_test_states():\n",
    "    from qutip import basis\n",
    "\n",
    "    # |0> state\n",
    "    initial_state = basis(2, 0).full()\n",
    "\n",
    "    # Superposition state\n",
    "    superposition_state = (basis(2, 0) + basis(2, 1)).unit().full()\n",
    "\n",
    "    # |1> state\n",
    "    final_state = basis(2, 1).full()\n",
    "\n",
    "    return [initial_state, superposition_state, final_state]\n",
    "\n",
    "\n",
    "# List of quantum states\n",
    "states = create_test_states()\n",
    "\n",
    "# Animate the trajectory of these states\n",
    "animate_bloch_trajectory(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: |psi_initial> and |psi_target> as numpy arrays\n",
    "psi_initial = np.array([0, 0, 0, 1])  # |11>\n",
    "psi_target = np.array([0, 0, 1, 0])  # |10>\n",
    "\n",
    "# Fidelity calculation\n",
    "fidelity = np.abs(np.dot(np.conjugate(psi_target), psi_initial)) ** 2\n",
    "print(f\"Fidelity: {fidelity:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the fidelity range from very low (close to 0) to almost 1\n",
    "fidelity = np.linspace(0, 1, 1000)  # Avoid exact 1 to prevent log(0)\n",
    "\n",
    "# Calculate the logarithmic infidelity\n",
    "log_infidelity = -np.log10(1 - fidelity)\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fidelity, log_infidelity, label=r\"$-\\log_{10}(1-F)$\", color=\"blue\")\n",
    "plt.xlabel(\"Fidelity (F)\")\n",
    "plt.ylabel(\"Logarithmic Infidelity\")\n",
    "plt.title(\"Logarithmic Infidelity as a Function of Fidelity\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_states = {\n",
    "    \"|00>\": np.array([1, 0, 0, 0]),\n",
    "    \"|01>\": np.array([0, 1, 0, 0]),\n",
    "    \"|10>\": np.array([0, 0, 1, 0]),\n",
    "    \"|11>\": np.array([0, 0, 0, 1]),\n",
    "}\n",
    "\n",
    "\n",
    "cnot_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],  # |00> -> |00>\n",
    "        [0, 1, 0, 0],  # |01> -> |01>\n",
    "        [0, 0, 0, 1],  # |10> -> |11>\n",
    "        [0, 0, 1, 0],  # |11> -> |10>\n",
    "    ]\n",
    ")\n",
    "\n",
    "transitions = [(\"|00>\", \"|00>\"), (\"|01>\", \"|01>\"), (\"|10>\", \"|11>\"), (\"|11>\", \"|10>\")]\n",
    "\n",
    "# Perform explicit fidelity calculations for each specified transition\n",
    "explicit_fidelities = {}\n",
    "for init_label, target_label in transitions:\n",
    "    # Retrieve initial and target states\n",
    "    init_state = basis_states[init_label]\n",
    "    target_state = basis_states[target_label]\n",
    "\n",
    "    # Apply CNOT to the initial state\n",
    "    final_state = np.dot(cnot_matrix, init_state)\n",
    "\n",
    "    # Perform the fidelity calculation explicitly\n",
    "    fidelity = np.abs(np.vdot(target_state, final_state)) ** 2  # |<target|final>|^2\n",
    "    explicit_fidelities[f\"{init_label} -> {target_label}\"] = fidelity\n",
    "\n",
    "explicit_fidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "states = [\n",
    "    np.array([[1, 0], [0, 1j]]),  # Identity with an imaginary element\n",
    "    np.array([[0, 1j], [1, 0]]),  # Swap gate with an imaginary element\n",
    "]\n",
    "\n",
    "# Process states\n",
    "processed_states = torch.FloatTensor(\n",
    "    np.array([np.concatenate([s.flatten().real, s.flatten().imag]) for s in states])\n",
    ")\n",
    "\n",
    "print(processed_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import logm\n",
    "\n",
    "\n",
    "# Define a helper function to calculate von Neumann entropy\n",
    "def von_neumann_entropy(density_matrix):\n",
    "    # Ensure the density matrix is Hermitian\n",
    "    eigenvalues = np.linalg.eigvalsh(density_matrix)\n",
    "    # Filter small eigenvalues for numerical stability\n",
    "    eigenvalues = eigenvalues[eigenvalues > 1e-10]\n",
    "    entropy = -np.sum(eigenvalues * np.log2(eigenvalues))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Define a Bell state density matrix\n",
    "bell_state = np.array([[1, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 1]]) / 2\n",
    "\n",
    "# Apply an Ry(theta) gate to the second qubit\n",
    "theta = np.pi / 4  # Example value for theta\n",
    "Ry = np.array(\n",
    "    [[np.cos(theta / 2), -np.sin(theta / 2)], [np.sin(theta / 2), np.cos(theta / 2)]]\n",
    ")\n",
    "\n",
    "# Tensor product of identity and Ry to act on the second qubit\n",
    "I = np.eye(2)\n",
    "operator = np.kron(I, Ry)\n",
    "\n",
    "# Updated density matrix after applying Ry(theta)\n",
    "updated_state = operator @ bell_state @ operator.T\n",
    "dims = (2, 2)\n",
    "\n",
    "# Trace out one qubit (partial trace to get reduced density matrix)\n",
    "def partial_trace(density_matrix, system_dim):\n",
    "    dim_A, dim_B = system_dim\n",
    "    reshaped_matrix = density_matrix.reshape([dim_A, dim_B, dim_A, dim_B])\n",
    "    return np.trace(reshaped_matrix, axis1=1, axis2=3)\n",
    "\n",
    "\n",
    "def compute_entropy_vs_theta():\n",
    "    thetas = np.linspace(0, 2 * np.pi, 100)  # Theta values from 0 to 2π\n",
    "    entropies = []\n",
    "\n",
    "    for theta in thetas:\n",
    "        # Define the Ry(theta) gate\n",
    "        Ry = np.array(\n",
    "            [\n",
    "                [np.cos(theta / 2), -np.sin(theta / 2)],\n",
    "                [np.sin(theta / 2), np.cos(theta / 2)],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Apply the gate to the Bell state\n",
    "        operator = np.kron(I, Ry)\n",
    "        updated_state = operator @ bell_state @ operator.T\n",
    "\n",
    "        # Compute the reduced density matrix\n",
    "        reduced_density_matrix = partial_trace(updated_state, dims)\n",
    "\n",
    "        # Compute the von Neumann entropy\n",
    "        entropy = von_neumann_entropy(reduced_density_matrix)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    return thetas, entropies\n",
    "\n",
    "\n",
    "# Compute entropy as a function of theta\n",
    "thetas, entropies = compute_entropy_vs_theta()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thetas, entropies, label=\"Von Neumann Entropy\")\n",
    "plt.title(\"Von Neumann Entropy vs. θ\")\n",
    "plt.xlabel(\"θ (radians)\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 0])\n",
    "b = np.array([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.kron(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadamard = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "I = np.eye(2)\n",
    "operator = np.kron(I, hadamard)\n",
    "updated_state = operator @ bell_state @ operator.T\n",
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnot = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])\n",
    "updated_state = cnot @ bell_state @ cnot.T\n",
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the sequence of operations: (Ry ⊗ I) CNOT (H ⊗ I) |00⟩\n",
    "import pandas as pd\n",
    "def compute_all_measures_corrected():\n",
    "    thetas = np.linspace(0, 2 * np.pi, 100)  # Theta values from 0 to 2π\n",
    "    entropies = []\n",
    "    purities = []\n",
    "    concurrences = []\n",
    "    mutual_informations = []\n",
    "\n",
    "    for theta in thetas:\n",
    "        # Define the gates\n",
    "        H = np.array([[1, 1], [1, -1]]) / np.sqrt(2)  # Hadamard gate\n",
    "        CNOT = np.array(\n",
    "            [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n",
    "        )  # CNOT gate\n",
    "        Ry = np.array(\n",
    "            [\n",
    "                [np.cos(theta / 2), -np.sin(theta / 2)],\n",
    "                [np.sin(theta / 2), np.cos(theta / 2)],\n",
    "            ]\n",
    "        )  # Ry(theta)\n",
    "\n",
    "        # Create the initial state |00⟩\n",
    "        initial_state = np.array([1, 0, 0, 0]).reshape(4, 1)\n",
    "\n",
    "        # Apply the gate sequence: (Ry ⊗ I) CNOT (H ⊗ I) |00⟩\n",
    "        H_I = np.kron(H, I)  # H ⊗ I\n",
    "        Ry_I = np.kron(Ry, I)  # Ry(theta) ⊗ I\n",
    "        final_state = Ry_I @ CNOT @ H_I @ initial_state\n",
    "        density_matrix = final_state @ final_state.T.conj()\n",
    "\n",
    "        # Compute the reduced density matrices\n",
    "        reduced_density_matrix_A = partial_trace(density_matrix, dims)\n",
    "        reduced_density_matrix_B = partial_trace(\n",
    "            density_matrix.T, dims\n",
    "        )  # Symmetric in this case\n",
    "\n",
    "        # Von Neumann entropy\n",
    "        entropy = von_neumann_entropy(reduced_density_matrix_A)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        # # Purity\n",
    "        # purity = calculate_purity(reduced_density_matrix_A)\n",
    "        # purities.append(purity)\n",
    "\n",
    "        # # Concurrence\n",
    "        # concurrence = calculate_concurrence(density_matrix)\n",
    "        # concurrences.append(concurrence)\n",
    "\n",
    "        # # Mutual information\n",
    "        # mutual_information = calculate_mutual_information(\n",
    "        #     reduced_density_matrix_A, reduced_density_matrix_B, density_matrix\n",
    "        # )\n",
    "        # mutual_informations.append(mutual_information)\n",
    "\n",
    "    return thetas, entropies, purities, concurrences, mutual_informations\n",
    "\n",
    "\n",
    "# Perform the corrected calculations\n",
    "(\n",
    "    thetas_corrected,\n",
    "    entropies_corrected,\n",
    "    purities_corrected,\n",
    "    concurrences_corrected,\n",
    "    mutual_informations_corrected,\n",
    ") = compute_all_measures_corrected()\n",
    "\n",
    "# Organize corrected results into a DataFrame for display\n",
    "corrected_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Theta (radians)\": thetas_corrected,\n",
    "        \"Von Neumann Entropy\": entropies_corrected,\n",
    "        # \"Purity\": purities_corrected,\n",
    "        # \"Concurrence\": concurrences_corrected,\n",
    "        # \"Mutual Information\": mutual_informations_corrected,\n",
    "    }\n",
    ")\n",
    "\n",
    "# tools.display_dataframe_to_user(\n",
    "#     name=\"Corrected Quantum Measures as a Function of Theta\",\n",
    "#     dataframe=corrected_results,\n",
    "# )\n",
    "\n",
    "# Plot all corrected measures for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thetas_corrected, entropies_corrected, label=\"Von Neumann Entropy\")\n",
    "# plt.plot(thetas_corrected, purities_corrected, label=\"Purity\")\n",
    "# plt.plot(thetas_corrected, concurrences_corrected, label=\"Concurrence\")\n",
    "# plt.plot(thetas_corrected, mutual_informations_corrected, label=\"Mutual Information\")\n",
    "plt.title(\"Corrected Quantum Measures vs. θ\")\n",
    "plt.xlabel(\"θ (radians)\")\n",
    "plt.ylabel(\"Measure Value\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.pi\n",
    "RY = np.array([[np.cos(theta/2), -np.sin(theta/2)], [np.sin(theta/2), np.cos(theta/2)]])\n",
    "operator = np.kron(I, RY)\n",
    "updated_state = operator @ updated_state @ operator.T\n",
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_state(states, gate_fidelity=False):\n",
    "        \"\"\"\n",
    "        Processes a batch of states for gate or state fidelity.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        states : list of tuple\n",
    "            A list of states, where each state is either:\n",
    "            - A tuple containing unitary matrices (for gate fidelity), or\n",
    "            - A tuple containing state vectors (for state fidelity).\n",
    "        is_gate_fidelity : bool\n",
    "            If True, process the states as unitary matrices (gate fidelity).\n",
    "            If False, process the states as state vectors (state fidelity).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor representation of the processed batch of states.\n",
    "        \"\"\"\n",
    "        processed_batch = []\n",
    "        for state in states:\n",
    "            processed_units = []\n",
    "            for element in state:\n",
    "                if gate_fidelity:\n",
    "                    # Process unitary matrix\n",
    "                    flattened_real = element.flatten().real\n",
    "                    flattened_imag = element.flatten().imag\n",
    "                    processed_units.append(\n",
    "                        np.concatenate([\n",
    "                            flattened_real,\n",
    "                            flattened_imag\n",
    "                        ])\n",
    "                    )\n",
    "                else:\n",
    "                    # Process state vector\n",
    "                    real_part = element.real\n",
    "                    imag_part = element.imag\n",
    "                    processed_units.append(\n",
    "                        np.concatenate([\n",
    "                            real_part,\n",
    "                            imag_part\n",
    "                        ])\n",
    "                    )\n",
    "            # Combine all processed units for this state\n",
    "            processed_batch.append(np.concatenate(processed_units))\n",
    "\n",
    "        # Convert batch to torch.Tensor\n",
    "        return torch.FloatTensor(np.array(processed_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitary1 = np.array([[1, 0], [0, -1]])\n",
    "unitary2 = np.array([[0, 1], [1, 0]])\n",
    "states = [(unitary1, unitary2)]  # Batch with one state\n",
    "processed = _process_state(states, gate_fidelity=True)\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = np.array([1 / np.sqrt(2), 1j / np.sqrt(2)])\n",
    "state2 = np.array([0, 1])\n",
    "states = [state1, state2]  # Batch with one state\n",
    "processed = _process_state(states, gate_fidelity=False)\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor(np.array([np.concatenate([s.real, s.imag]) for s in states]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "# Example Hamiltonian (2-qubit system)\n",
    "H = np.array([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "\n",
    "# Time step\n",
    "t = 0\n",
    "\n",
    "\n",
    "# Compute time evolution operator\n",
    "def _time_evolution_operator(H, t):\n",
    "    return expm(-1j * H * t)\n",
    "\n",
    "\n",
    "U = _time_evolution_operator(H, t)\n",
    "print(\"Time Evolution Operator U(t):\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.linalg import expm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define Hamiltonian parameters\n",
    "class Hamiltonian:\n",
    "    def __init__(self, omega_max, delta_max, J_max):\n",
    "        self.omega_max = omega_max\n",
    "        self.delta_max = delta_max\n",
    "        self.J_max = J_max\n",
    "\n",
    "    def compute(self, omega, delta, J, t):\n",
    "        X = np.array([[0, 1], [1, 0]])\n",
    "        Y = np.array([[0, -1j], [1j, 0]])\n",
    "        Z = np.array([[1, 0], [0, -1]])\n",
    "        I = np.eye(2)\n",
    "\n",
    "        H1 = 0.5 * (omega * np.cos(t) * X + omega * np.sin(t) * Y + delta * Z)\n",
    "        H2 = 0.5 * (omega * np.cos(t) * X + omega * np.sin(t) * Y + delta * Z)\n",
    "        return np.kron(H1, I) + np.kron(I, H2) + 0.5 * J * np.kron(Z, Z)\n",
    "\n",
    "\n",
    "# DRL agent\n",
    "class DRLAgent(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DRLAgent, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_agent(agent, env, episodes, learning_rate):\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "    # Lists to store metrics\n",
    "    all_rewards = []\n",
    "    all_losses = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "\n",
    "        while not done:\n",
    "            # Convert state to PyTorch tensor\n",
    "            # Normalize the state before feeding it to the agent\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            state_tensor = (state_tensor - state_tensor.mean()) / (state_tensor.std() + 1e-5)\n",
    "\n",
    "\n",
    "            # Get action from the agent\n",
    "            action = agent(state_tensor).detach().numpy()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward  # Accumulate total reward for the episode\n",
    "\n",
    "            # Convert reward to PyTorch tensor\n",
    "            reward_tensor = torch.tensor(\n",
    "                reward, dtype=torch.float32, requires_grad=True\n",
    "            )\n",
    "\n",
    "            # Define loss and optimize policy\n",
    "            loss = -reward_tensor  # Negative reward as loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss\n",
    "            episode_losses.append(loss.item())\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "        # Store metrics for the episode\n",
    "        all_rewards.append(total_reward)\n",
    "        all_losses.append(sum(episode_losses) / len(episode_losses))  # Average loss\n",
    "\n",
    "    return all_rewards, all_losses\n",
    "\n",
    "\n",
    "# Environment simulation\n",
    "class QuantumEnv:\n",
    "    def __init__(self, hamiltonian, steps, omega_max, delta_max, J_max):\n",
    "        self.hamiltonian = hamiltonian\n",
    "        self.steps = steps\n",
    "        self.omega_max = omega_max\n",
    "        self.delta_max = delta_max\n",
    "        self.J_max = J_max\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.propagator = np.eye(4, dtype=complex)\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return np.hstack(\n",
    "            [self.propagator.real.flatten(), self.propagator.imag.flatten()]\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        omega, delta, J = action\n",
    "        H = self.hamiltonian.compute(\n",
    "            omega * self.omega_max, delta * self.delta_max, J * self.J_max, self.t\n",
    "        )\n",
    "        self.propagator = expm(-1j * H * (1 / self.steps)) @ self.propagator\n",
    "        self.t += 1 / self.steps\n",
    "        done = self.t >= 1\n",
    "        reward = self._get_reward()\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "    def _get_reward(self):\n",
    "        target_gate = expm(\n",
    "            -1j\n",
    "            * np.pi\n",
    "            / 4\n",
    "            * np.kron(np.array([[0, -1j], [1j, 0]]), np.array([[0, -1j], [1j, 0]]))\n",
    "        )\n",
    "        fidelity = np.abs(np.trace(target_gate.conj().T @ self.propagator) / 4) ** 2\n",
    "        return 1 - fidelity\n",
    "\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Define parameters\n",
    "    omega_max = 2 * np.pi\n",
    "    delta_max = 2 * np.pi\n",
    "    J_max = 2 * np.pi\n",
    "    steps = 20\n",
    "\n",
    "    hamiltonian = Hamiltonian(omega_max, delta_max, J_max)\n",
    "    env = QuantumEnv(hamiltonian, steps, omega_max, delta_max, J_max)\n",
    "    agent = DRLAgent(input_dim=32, output_dim=3)  # Match input_dim to state vector size\n",
    "\n",
    "    # Train the agent\n",
    "    episodes = 10000\n",
    "    learning_rate = 1e-4\n",
    "    rewards, losses = train_agent(agent, env, episodes, learning_rate)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(episodes), rewards, label=\"Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(episodes), losses, label=\"Loss\", color=\"red\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.linalg import expm\n",
    "\n",
    "\n",
    "# Define Hamiltonian\n",
    "class Hamiltonian:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute(self, omega, delta, J, t):\n",
    "        X = np.array([[0, 1], [1, 0]])\n",
    "        Y = np.array([[0, -1j], [1j, 0]])\n",
    "        Z = np.array([[1, 0], [0, -1]])\n",
    "        I = np.eye(2)\n",
    "\n",
    "        H1 = 0.5 * (omega * np.cos(t) * X + omega * np.sin(t) * Y + delta * Z)\n",
    "        H2 = 0.5 * (omega * np.cos(t) * X + omega * np.sin(t) * Y + delta * Z)\n",
    "        return np.kron(H1, I) + np.kron(I, H2) + 0.5 * J * np.kron(Z, Z)\n",
    "\n",
    "\n",
    "# Define RL Agent\n",
    "class RLAgent(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.1):\n",
    "        super(RLAgent, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Define Environment\n",
    "class QuantumEnv:\n",
    "    def __init__(self, hamiltonian, steps, omega_max, delta_max, J_max):\n",
    "        self.hamiltonian = hamiltonian\n",
    "        self.steps = steps\n",
    "        self.omega_max = omega_max\n",
    "        self.delta_max = delta_max\n",
    "        self.J_max = J_max\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.propagator = np.eye(4, dtype=complex)\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return np.hstack(\n",
    "            [self.propagator.real.flatten(), self.propagator.imag.flatten()]\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        omega, delta, J = action\n",
    "        H = self.hamiltonian.compute(\n",
    "            omega * self.omega_max, delta * self.delta_max, J * self.J_max, self.t\n",
    "        )\n",
    "        self.propagator = expm(-1j * H * (1 / self.steps)) @ self.propagator\n",
    "        self.t += 1 / self.steps\n",
    "        done = self.t >= 1\n",
    "        reward = self._get_reward()\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "    def _get_reward(self):\n",
    "        target_gate = expm(\n",
    "            -1j\n",
    "            * np.pi\n",
    "            / 4\n",
    "            * np.kron(np.array([[0, -1j], [1j, 0]]), np.array([[0, -1j], [1j, 0]]))\n",
    "        )\n",
    "        fidelity = np.abs(np.trace(target_gate.conj().T @ self.propagator) / 4) ** 2\n",
    "        return -np.log10(1 - fidelity)\n",
    "\n",
    "\n",
    "# Train the RL Agent\n",
    "def train_agent(agent, env, episodes, learning_rate):\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "    all_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, requires_grad=False)\n",
    "            action = agent(state_tensor).detach().numpy()\n",
    "            action += np.random.normal(\n",
    "                0, 0.1, size=action.shape\n",
    "            )  # Gaussian perturbation\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward  # Accumulate the total reward for the episode\n",
    "\n",
    "            # Use the reward to calculate the loss\n",
    "            loss = -torch.tensor(reward, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    omega_max, delta_max, J_max = 2 * np.pi, 2 * np.pi, 2 * np.pi\n",
    "    steps = 20\n",
    "    hamiltonian = Hamiltonian()\n",
    "    env = QuantumEnv(hamiltonian, steps, omega_max, delta_max, J_max)\n",
    "    agent = RLAgent(input_dim=32, output_dim=3)\n",
    "\n",
    "    episodes = 10000\n",
    "    learning_rate = 1e-5\n",
    "    rewards = train_agent(agent, env, episodes, learning_rate)\n",
    "\n",
    "    # Plot rewards\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(range(episodes), rewards)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"RL Agent Training Rewards\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the RL agent as a neural network\n",
    "class RLAgent(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, dropout_prob=0.1):\n",
    "        super(RLAgent, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "# Define the environment\n",
    "class QuantumEnvironment:\n",
    "    def __init__(self, target_gate, perturbation_std=0.01):\n",
    "        self.target_gate = target_gate  # e.g., R_YY(pi/4)\n",
    "        self.perturbation_std = perturbation_std\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.U = np.eye(4, dtype=np.complex128)  # Initial propagator\n",
    "        self.time_step = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        real_part = self.U.real.flatten()\n",
    "        imag_part = self.U.imag.flatten()\n",
    "        normalized_time = np.array([self.time_step / 100.0])\n",
    "        return np.concatenate([real_part, imag_part, normalized_time])\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply Gaussian perturbation to simulate noise\n",
    "        perturbed_action = action + np.random.normal(0, self.perturbation_std, size=action.shape)\n",
    "\n",
    "        # Construct a unitary evolution matrix from the perturbed action\n",
    "        # For simplicity, assume action contains amplitudes for Pauli operators (X, Y, Z)\n",
    "        omega_x, omega_y, omega_z = perturbed_action\n",
    "        unitary = np.array([\n",
    "            [np.cos(omega_z) - 1j * np.sin(omega_z), 0],\n",
    "            [0, np.cos(omega_z) + 1j * np.sin(omega_z)]\n",
    "        ])  # Replace this with your actual dynamics model if necessary\n",
    "\n",
    "        # Update the propagator\n",
    "        self.U = self.U @ unitary  # Matrix multiplication\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Reward: similarity to the target gate\n",
    "        reward = -np.linalg.norm(self.U - self.target_gate)\n",
    "        done = self.time_step >= 100  # End episode after 100 steps\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_rl_agent(env, agent, optimizer, criterion, num_episodes=500):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(100):  # Max steps per episode\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action = agent(state_tensor).detach().numpy()  # Forward pass\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Optimize the agent\n",
    "            optimizer.zero_grad()\n",
    "            predicted_action = agent(state_tensor)\n",
    "            loss = criterion(\n",
    "                predicted_action, torch.tensor(action, dtype=torch.float32)\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_agent(env, agent):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(100):  # Max steps\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action = agent(state_tensor).detach().numpy()  # Forward pass\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Evaluation Total Reward: {total_reward}\")\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = 33  # Example: real + imag parts of 4x4 propagator + normalized time\n",
    "    output_dim = 3  # Example: control amplitudes, phases, interaction terms\n",
    "    agent = RLAgent(input_dim, output_dim)\n",
    "\n",
    "    # Define the target gate (e.g., R_YY(pi/4))\n",
    "    target_gate = np.array(\n",
    "        [[1, 0, 0, 0], [0, 0, -1j, 0], [0, -1j, 0, 0], [0, 0, 0, 1]],\n",
    "        dtype=np.complex128,\n",
    "    )\n",
    "\n",
    "    env = QuantumEnvironment(target_gate=target_gate)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train the agent\n",
    "    train_rl_agent(env, agent, optimizer, criterion, num_episodes=100)\n",
    "\n",
    "    # Evaluate the agent in a noise-free environment\n",
    "    noise_free_env = QuantumEnvironment(target_gate=target_gate, perturbation_std=0.0)\n",
    "    evaluate_agent(noise_free_env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m model \u001b[38;5;241m=\u001b[39m ActorCritic(input_dim, action_dim)\n\u001b[1;32m    226\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 125\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env, model, optimizer, num_episodes, gamma, epsilon, c1, c2)\u001b[0m\n\u001b[1;32m    123\u001b[0m action \u001b[38;5;241m=\u001b[39m action_dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    124\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m action_dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[0;32m--> 125\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[1;32m    128\u001b[0m values\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m, in \u001b[0;36mQuantumEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Denormalize actions\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     Omega1, Delta1, Omega2, Delta2, J \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     54\u001b[0m     Omega1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOmega_max\n\u001b[1;32m     55\u001b[0m     Delta1 \u001b[38;5;241m=\u001b[39m Delta1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDelta_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDelta_max\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.linalg import expm\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "# Define the PPO Actor-Critic model\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Common network\n",
    "        self.shared = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())\n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        shared_out = self.shared(state)\n",
    "        return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "\n",
    "# Define the QuantumEnvironment\n",
    "class QuantumEnvironment:\n",
    "    def __init__(\n",
    "        self, target_gate, T=1.0, N_max=100, Omega_max=1.0, Delta_max=1.0, J_max=1.0\n",
    "    ):\n",
    "        self.target_gate = target_gate\n",
    "        self.T = T\n",
    "        self.N_max = N_max\n",
    "        self.delta_T = T / N_max\n",
    "        self.Omega_max = Omega_max\n",
    "        self.Delta_max = Delta_max\n",
    "        self.J_max = J_max\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.U = np.eye(4, dtype=np.complex128)\n",
    "        self.time_step = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Denormalize actions\n",
    "        Omega1, Delta1, Omega2, Delta2, J = action\n",
    "        Omega1 *= self.Omega_max\n",
    "        Delta1 = Delta1 * 2 * self.Delta_max - self.Delta_max\n",
    "        Omega2 *= self.Omega_max\n",
    "        Delta2 = Delta2 * 2 * self.Delta_max - self.Delta_max\n",
    "        J *= self.J_max\n",
    "\n",
    "        # Define Hamiltonian\n",
    "        H1 = Omega1 * np.kron(self._pauli_x(), np.eye(2)) + Delta1 * np.kron(\n",
    "            self._pauli_z(), np.eye(2)\n",
    "        )\n",
    "        H2 = Omega2 * np.kron(np.eye(2), self._pauli_x()) + Delta2 * np.kron(\n",
    "            np.eye(2), self._pauli_z()\n",
    "        )\n",
    "        H_interaction = J * np.kron(self._pauli_z(), self._pauli_z())\n",
    "        H_total = H1 + H2 + H_interaction\n",
    "\n",
    "        # Propagator update\n",
    "        self.U = self.U @ expm(-1j * H_total * self.delta_T)\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Reward calculation\n",
    "        done = self.time_step >= self.N_max\n",
    "        reward = 0\n",
    "        if done:\n",
    "            reward = self._calculate_reward()\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "    def _get_state(self):\n",
    "        real_part = self.U.real.flatten()\n",
    "        imag_part = self.U.imag.flatten()\n",
    "        normalized_time = np.array([self.time_step / self.N_max])\n",
    "        return np.concatenate([real_part, imag_part, normalized_time])\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        fidelity = self._calculate_fidelity()\n",
    "        return -np.log10(1 - fidelity)\n",
    "\n",
    "    def _calculate_fidelity(self):\n",
    "        dim = self.U.shape[0]\n",
    "        overlap = np.trace(self.target_gate.conj().T @ self.U)\n",
    "        fidelity = np.abs(overlap) ** 2 / dim\n",
    "        return fidelity\n",
    "\n",
    "    @staticmethod\n",
    "    def _pauli_x():\n",
    "        return np.array([[0, 1], [1, 0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _pauli_y():\n",
    "        return np.array([[0, -1j], [1j, 0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _pauli_z():\n",
    "        return np.array([[1, 0], [0, -1]])\n",
    "\n",
    "\n",
    "# PPO training function\n",
    "def train_ppo(\n",
    "    env, model, optimizer, num_episodes=1000, gamma=0.99, epsilon=0.2, c1=1.0, c2=0.01\n",
    "):\n",
    "    for episode in range(num_episodes):\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "        log_probs, values, rewards, states, actions = [], [], [], [], []\n",
    "\n",
    "        # Collect trajectory\n",
    "        done = False\n",
    "        while not done:\n",
    "            policy, value = model(state)\n",
    "            action_dist = Categorical(policy)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            next_state, reward, done = env.step(action.numpy())\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Compute advantages and returns\n",
    "        returns, advantages = compute_gae(rewards, values, gamma)\n",
    "\n",
    "        # Update policy and value network\n",
    "        for _ in range(4):  # PPO multiple updates\n",
    "            update_ppo(\n",
    "                model,\n",
    "                optimizer,\n",
    "                states,\n",
    "                actions,\n",
    "                log_probs,\n",
    "                values,\n",
    "                returns,\n",
    "                advantages,\n",
    "                epsilon,\n",
    "                c1,\n",
    "                c2,\n",
    "            )\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {sum(rewards):.3f}\")\n",
    "\n",
    "\n",
    "def compute_gae(rewards, values, gamma, tau=0.95):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    next_value = 0\n",
    "    gae = 0\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * next_value - values[step].detach()\n",
    "        gae = delta + gamma * tau * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value = values[step].detach()\n",
    "        returns.insert(0, gae + values[step].detach())\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    return returns, advantages\n",
    "\n",
    "\n",
    "def update_ppo(\n",
    "    model,\n",
    "    optimizer,\n",
    "    states,\n",
    "    actions,\n",
    "    log_probs,\n",
    "    values,\n",
    "    returns,\n",
    "    advantages,\n",
    "    epsilon,\n",
    "    c1,\n",
    "    c2,\n",
    "):\n",
    "    new_log_probs, new_values = [], []\n",
    "    for state, action in zip(states, actions):\n",
    "        policy, value = model(state)\n",
    "        action_dist = Categorical(policy)\n",
    "        new_log_probs.append(action_dist.log_prob(action))\n",
    "        new_values.append(value)\n",
    "\n",
    "    new_log_probs = torch.stack(new_log_probs)\n",
    "    new_values = torch.stack(new_values)\n",
    "\n",
    "    ratios = torch.exp(new_log_probs - torch.stack(log_probs))\n",
    "    surrogate1 = ratios * advantages\n",
    "    surrogate2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "    critic_loss = c1 * (returns - new_values).pow(2).mean()\n",
    "    entropy_loss = c2 * -torch.stack([dist.entropy() for dist in new_log_probs]).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss + entropy_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_gate = expm(\n",
    "        -1j\n",
    "        * (\n",
    "            np.kron(np.array([[0, -1j], [1j, 0]]), np.array([[0, -1j], [1j, 0]]))\n",
    "            * np.pi\n",
    "            / 4\n",
    "        )\n",
    "    )\n",
    "    env = QuantumEnvironment(target_gate=target_gate)\n",
    "\n",
    "    input_dim = 33  # 16 real, 16 imag, 1 normalized time\n",
    "    action_dim = 5  # Omega1, Delta1, Omega2, Delta2, J\n",
    "    model = ActorCritic(input_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_ppo(env, model, optimizer, num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "H = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "final_unitary=np.eye(2)\n",
    "dim = final_unitary.shape[0]  # Dimension of the unitary matrix\n",
    "# Fidelity calculation\n",
    "overlap = np.trace(np.dot(H.conj().T, final_unitary))\n",
    "fidelity = np.abs(overlap) ** 2 / dim\n",
    "fidelity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.70710678,  0.70710678],\n",
       "        [ 0.70710678, -0.70710678]]),\n",
       " 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.conj().T, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is final_unitary unitary? True\n",
      "Is target_unitary unitary? True\n",
      "fidelity 0.6294423457075998\n",
      "Final Unitary:\n",
      " [[ 0.58468829+0.63754837j  0.13029964+0.48445194j]\n",
      " [-0.13029964+0.48445194j  0.58468829-0.63754837j]]\n",
      "Target Unitary:\n",
      " [[ 0.70710678+0.j  0.70710678+0.j]\n",
      " [ 0.70710678+0.j -0.70710678+0.j]]\n",
      "Trace of Final Unitary:\n",
      " (1.16937658+0j)\n",
      "Trace of Target Unitary:\n",
      " 0j\n",
      "Overlap:\n",
      " 1.5867480527262037j\n",
      "Fidelity:\n",
      " 0.6294423457075998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gate_fidelity(final_unitary, target_unitary):\n",
    "    \"\"\"\n",
    "    Calculate the fidelity between the accumulated propagator and the target unitary.\n",
    "\n",
    "    Parameters:\n",
    "        final_unitary (np.ndarray): The accumulated propagator matrix (unitary).\n",
    "        target_unitary (np.ndarray): The target unitary matrix.\n",
    "\n",
    "    Returns:\n",
    "        float: The fidelity value.\n",
    "    \"\"\"\n",
    "    dim = final_unitary.shape[0]  # Dimension of the unitary matrix\n",
    "\n",
    "    # Fidelity calculation\n",
    "    overlap = np.trace(np.dot(target_unitary.conj().T, final_unitary))\n",
    "    fidelity = np.abs(overlap/dim) ** 2\n",
    "    return fidelity\n",
    "\n",
    "\n",
    "# Given matrices\n",
    "final_unitary = np.array(\n",
    "    [\n",
    "        [0.58468829 + 0.63754837j, 0.13029964 + 0.48445194j],\n",
    "        [-0.13029964 + 0.48445194j, 0.58468829 - 0.63754837j],\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_unitary = np.array(\n",
    "    [[0.70710678 + 0.0j, 0.70710678 + 0.0j], [0.70710678 + 0.0j, -0.70710678 + 0.0j]]\n",
    ")\n",
    "\n",
    "\n",
    "def is_unitary(matrix):\n",
    "    return np.allclose(np.eye(matrix.shape[0]), matrix.conj().T @ matrix)\n",
    "\n",
    "\n",
    "print(\"Is final_unitary unitary?\", is_unitary(final_unitary))\n",
    "print(\"Is target_unitary unitary?\", is_unitary(target_unitary))\n",
    "\n",
    "\n",
    "# Calculate fidelity\n",
    "fidelity = gate_fidelity(final_unitary, target_unitary)\n",
    "print('fidelity', fidelity.item())  # Print fidelity\n",
    "def debug_fidelity(final_unitary, target_unitary):\n",
    "    print(\"Final Unitary:\\n\", final_unitary)\n",
    "    print(\"Target Unitary:\\n\", target_unitary)\n",
    "    print(\"Trace of Final Unitary:\\n\", np.trace(final_unitary))\n",
    "    print(\"Trace of Target Unitary:\\n\", np.trace(target_unitary))\n",
    "    print(\"Overlap:\\n\", np.trace(np.dot(target_unitary.conj().T, final_unitary)))\n",
    "    print(\"Fidelity:\\n\", gate_fidelity(final_unitary, target_unitary))\n",
    "debug_fidelity(final_unitary, target_unitary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
